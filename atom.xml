<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>欢迎来到我的hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-11-27T13:28:09.136Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>二分法</title>
    <link href="http://yoursite.com/2018/11/27/%E4%BA%8C%E5%88%86%E6%B3%95/"/>
    <id>http://yoursite.com/2018/11/27/二分法/</id>
    <published>2018-11-27T13:06:43.000Z</published>
    <updated>2018-11-27T13:28:09.136Z</updated>
    
    <content type="html"><![CDATA[<h1 id="二分法求根推导"><a href="#二分法求根推导" class="headerlink" title="二分法求根推导"></a>二分法求根推导</h1><p>二分法其实就是确定某一个区间[a,b]内必定存在一个根，我们需要每次更新区间使得我们找到所需要找到德根</p><h2 id="1-0-截止"><a href="#1-0-截止" class="headerlink" title="1.0 截止"></a>1.0 截止</h2><p>截止条件:</p><p>   <em>1</em> 迭代一定的次数之后截止</p><p>   <em>2</em> 真实误差小于一定值之后截止<a href="https://www.codecogs.com/eqnedit.php?latex=(x^{new}-&space;x^{old})/x^{new}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(x^{new}-&space;x^{old})/x^{new}" title="(x^{new}- x^{old})/x^{new}"></a></p><p>   <em>3</em> 区间值小于一定值之后截止  例：abs(b-a)&lt;5e-5</p><h2 id="2-0-流程"><a href="#2-0-流程" class="headerlink" title="2.0 流程"></a>2.0 流程</h2><p>实现流程图</p><p><img src="https://i.imgur.com/qdEp6uw.png" alt=""></p><p>我们每次的迭代都需要判断x的值在哪个区间内，然后更新区间，如此下去知道截止条件出现</p><p>下面是我GitHub上面实现的python代码链接</p><p><a href="https://github.com/hedeqing/DataAnalysis" title="github" target="_blank" rel="noopener">https://github.com/hedeqing/DataAnalysis</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;二分法求根推导&quot;&gt;&lt;a href=&quot;#二分法求根推导&quot; class=&quot;headerlink&quot; title=&quot;二分法求根推导&quot;&gt;&lt;/a&gt;二分法求根推导&lt;/h1&gt;&lt;p&gt;二分法其实就是确定某一个区间[a,b]内必定存在一个根，我们需要每次更新区间使得我们找到所需要找到德
      
    
    </summary>
    
    
      <category term="notes" scheme="http://yoursite.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>java_EE入门之Html</title>
    <link href="http://yoursite.com/2018/10/23/java-EE%E5%85%A5%E9%97%A8/"/>
    <id>http://yoursite.com/2018/10/23/java-EE入门/</id>
    <published>2018-10-23T10:28:15.000Z</published>
    <updated>2018-11-25T02:42:43.047Z</updated>
    
    <content type="html"><![CDATA[<p>#第一天</p><h2 id="1-0-html是什么"><a href="#1-0-html是什么" class="headerlink" title="1.0 html是什么"></a>1.0 html是什么</h2><h3 id="1-0-超文本标记语言（超出文本操作的范畴，标记就是标签-lt-标签名-gt-）"><a href="#1-0-超文本标记语言（超出文本操作的范畴，标记就是标签-lt-标签名-gt-）" class="headerlink" title="1.0 超文本标记语言（超出文本操作的范畴，标记就是标签&lt;标签名&gt;）"></a>1.0 超文本标记语言（超出文本操作的范畴，标记就是标签&lt;标签名&gt;）</h3><h3 id="2-0-网页语言"><a href="#2-0-网页语言" class="headerlink" title="2.0 网页语言"></a>2.0 网页语言</h3><h2 id="2-0-创建一个文件-hello-html"><a href="#2-0-创建一个文件-hello-html" class="headerlink" title="2.0 创建一个文件 hello.html"></a>2.0 创建一个文件 hello.html</h2><pre><code>&lt;font size = &quot;4&quot; color =&quot;red&quot;&gt;这是我的第一个html程序&lt;/font&gt;</code></pre><blockquote><p>font 是标签，size，color是他的属性</p></blockquote><h2 id="3-0规范"><a href="#3-0规范" class="headerlink" title="3.0规范"></a>3.0规范</h2><h3 id="1-0-html文件包含开始和结束的标签-lt-html-gt-lt-html-gt"><a href="#1-0-html文件包含开始和结束的标签-lt-html-gt-lt-html-gt" class="headerlink" title="1.0 html文件包含开始和结束的标签&lt;html&gt;  &lt;/html&gt;"></a>1.0 html文件包含开始和结束的标签&lt;html&gt;  &lt;/html&gt;</h3><h3 id="2-0-lt-head-gt-设置相关信息-lt-head-gt"><a href="#2-0-lt-head-gt-设置相关信息-lt-head-gt" class="headerlink" title="2.0 &lt;head&gt;设置相关信息&lt;/head&gt;"></a>2.0 &lt;head&gt;设置相关信息&lt;/head&gt;</h3><h3 id="3-0-lt-body-gt-显示在页面上的内容都写在body里面-lt-body-gt"><a href="#3-0-lt-body-gt-显示在页面上的内容都写在body里面-lt-body-gt" class="headerlink" title="3.0  &lt;body&gt;显示在页面上的内容都写在body里面&lt;/body&gt;"></a>3.0  &lt;body&gt;显示在页面上的内容都写在body里面&lt;/body&gt;</h3><h3 id="4-0-html标签有开始就有结束"><a href="#4-0-html标签有开始就有结束" class="headerlink" title="4.0 html标签有开始就有结束"></a>4.0 html标签有开始就有结束</h3><h3 id="5-0-html有些标签没有结束标签，就在标签内结束"><a href="#5-0-html有些标签没有结束标签，就在标签内结束" class="headerlink" title="5.0 html有些标签没有结束标签，就在标签内结束"></a>5.0 html有些标签没有结束标签，就在标签内结束</h3><p>如换行&lt;br/&gt;  ， 下划线&lt;hr/&gt;</p><h2 id="4-0-html的操作思想"><a href="#4-0-html的操作思想" class="headerlink" title="4.0 html的操作思想"></a>4.0 html的操作思想</h2><p>网页中不同数需要不同的表现效果，这时候就可以通过标签吧他们封装起来，进而修改标签的属性可以达到我们需要的效果</p><h2 id="5-0-html常用的标签"><a href="#5-0-html常用的标签" class="headerlink" title="5.0 html常用的标签"></a>5.0 html常用的标签</h2><p>*文字标签</p><ul><li><p>&lt;font&gt;   &lt;/font&gt;</p></li><li><p>属性 </p><ul><li>size : 文字的大小，取值1-7，超出了7默认还是7。</li><li>coclr ： 两种表达方式，使用英文字母：red,green等等，或者16进制数来表述 #fffff （可通过工具来实现）</li></ul></li></ul><p>*注释标签<br>-&lt;!–html–&gt;</p><h2 id="6-0-标题标签，水平线标签，和特殊字符"><a href="#6-0-标题标签，水平线标签，和特殊字符" class="headerlink" title="6.0 标题标签，水平线标签，和特殊字符"></a>6.0 标题标签，水平线标签，和特殊字符</h2><p>*标题标签</p><p> &lt;h&gt;  &lt;/h1&gt;  ..&lt;h2&gt;  &lt;/h2&gt;..<br>&lt;h3&gt;  &lt;/h3&gt;..<br>&lt;h4&gt;  &lt;/h4&gt;..<br>&lt;h5&gt;  &lt;/h5&gt;..<br>&lt;h6&gt;  &lt;/h6&gt;..</p><p>实现标题的大小的依次变小，自动换行</p><p>*水平线标签</p><ul><li><p>&lt;hr/&gt;</p></li><li><p>属性</p><ul><li><p>size：水平线的粗细，取值1-7</p></li><li><p>color ： 水平线的颜色</p></li></ul></li></ul><p>*特殊字符<br><img src="https://i.imgur.com/3XREXFw.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;#第一天&lt;/p&gt;
&lt;h2 id=&quot;1-0-html是什么&quot;&gt;&lt;a href=&quot;#1-0-html是什么&quot; class=&quot;headerlink&quot; title=&quot;1.0 html是什么&quot;&gt;&lt;/a&gt;1.0 html是什么&lt;/h2&gt;&lt;h3 id=&quot;1-0-超文本标记语言（超出文本操
      
    
    </summary>
    
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow笔记</title>
    <link href="http://yoursite.com/2018/10/21/tensorflow%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/10/21/tensorflow笔记/</id>
    <published>2018-10-21T07:11:20.000Z</published>
    <updated>2018-10-21T13:39:31.391Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-tensorflow"><a href="#1-tensorflow" class="headerlink" title="1. tensorflow"></a>1. tensorflow</h1><h2 id="1-1-神经网络-python3-5"><a href="#1-1-神经网络-python3-5" class="headerlink" title="1.1 神经网络(python3.5)"></a>1.1 神经网络(python3.5)</h2><h3 id="1-2-参数：权重W"><a href="#1-2-参数：权重W" class="headerlink" title="1.2 参数：权重W"></a>1.2 参数：权重W</h3><p>线上的权重w，用变量表示，随机给初始值</p><pre><code>import tensorflow as tfw = tf.Variable(tf.random_normal([2,3],stddev = 2,mean = 0,seed =1))</code></pre><p>stddev: 标准差，mean: 均值,seed: 随机种子.</p><p>tf.random_normal标准正太分布，tf.truncated_normal 去掉过大偏离点的正态分布，tf.random_uniform 平均分布<br>也可以给定值</p><pre><code>tf.zeros()#全0数组 tf.zeros([3,2],tf.int32)tf.ones()#全1数组    tf.ones([3,2],tf.int32)tf.fill()#全定值数组   tf.fill([3,2],2)tf.constant()#直接赋值  tf.constant([2.0,3.0])</code></pre><h3 id="1-3-喂数据-op，运行的节点"><a href="#1-3-喂数据-op，运行的节点" class="headerlink" title="1.3 喂数据(op，运行的节点)"></a>1.3 喂数据(op，运行的节点)</h3><pre><code>sess = tf.Session()init = tf.global_variable_initlizer()sess.run(init) x = tf.placeholder(tf.float,shape = (None,2))sess.run(op,feed_dic={x:[[0.5,0.6]]})    </code></pre><h3 id="1-4-前向传播"><a href="#1-4-前向传播" class="headerlink" title="1.4 前向传播"></a>1.4 前向传播</h3><h3 id="1-5-反向传播-更新参数"><a href="#1-5-反向传播-更新参数" class="headerlink" title="1.5 反向传播(更新参数)"></a>1.5 反向传播(更新参数)</h3><h4 id="1-5-1-损失函数"><a href="#1-5-1-损失函数" class="headerlink" title="1.5.1 损失函数"></a>1.5.1 损失函数</h4><p>预测值y与真实值y_之间的差值</p><h4 id="1-5-2-均方误差：MSE"><a href="#1-5-2-均方误差：MSE" class="headerlink" title="1.5.2 均方误差：MSE"></a>1.5.2 均方误差：MSE</h4><pre><code>loss = tf.reduce_mean(tf.square(y_-y))</code></pre><h4 id="1-5-3-训练方法：以减少loss为优化目标"><a href="#1-5-3-训练方法：以减少loss为优化目标" class="headerlink" title="1.5.3 训练方法：以减少loss为优化目标"></a>1.5.3 训练方法：以减少loss为优化目标</h4><pre><code>trin_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)train_op = tf.train.MomentumOptimizer(learning_rate).minimize(loss)train_op = tf.train.AdamOptimizer(leanring_rate).minimize(loss)</code></pre><h4 id="1-5-4-学习率"><a href="#1-5-4-学习率" class="headerlink" title="1.5.4 学习率"></a>1.5.4 学习率</h4><p>决定参数每次更新的幅度</p><h3 id="1-6-激活函数"><a href="#1-6-激活函数" class="headerlink" title="1.6 激活函数"></a>1.6 激活函数</h3><pre><code>tf.nn.relu()tf.nn.sigmoid()tf.nn.tanh()</code></pre><h3 id="1-7-NN复杂度"><a href="#1-7-NN复杂度" class="headerlink" title="1.7 NN复杂度"></a>1.7 NN复杂度</h3><p>层数 = 隐藏层的层数+1个输出层</p><p>总参数 =  总w（权重个数） +总b（偏置项个数）</p><h3 id="1-8-模型优化"><a href="#1-8-模型优化" class="headerlink" title="1.8 模型优化"></a>1.8 模型优化</h3><h4 id="1-8-1-损失函数loss，学习率learning-rate-滑动平均-ema-正则化regularization"><a href="#1-8-1-损失函数loss，学习率learning-rate-滑动平均-ema-正则化regularization" class="headerlink" title="1.8.1 损失函数loss，学习率learning_rate,滑动平均 ema,正则化regularization"></a>1.8.1 损失函数loss，学习率learning_rate,滑动平均 ema,正则化regularization</h4><h5 id="1-8-1-1-自定义损失函数"><a href="#1-8-1-1-自定义损失函数" class="headerlink" title="1.8.1.1 自定义损失函数"></a>1.8.1.1 自定义损失函数</h5><p>不知道y 与y_的大小时</p><pre><code>loss = tf.reduce_mean(tf.where(tf.greater(y,y_),COST(y -y_),PROFIT(y_-y)))</code></pre><h5 id="1-8-1-2-交叉熵-cem"><a href="#1-8-1-2-交叉熵-cem" class="headerlink" title="1.8.1.2 交叉熵 cem"></a>1.8.1.2 交叉熵 cem</h5><p>交叉熵越小，两个概率分布越近，越大，两个概率分布更远</p><pre><code>H= -Σ（y_*log(y)）</code></pre><p>如二分类答案 y = (1,0),预测y1 = (0.6,0.4),y2 = (0.9,0.2)哪个更接近正确答案呢？</p><p>H1（（1，0）（0.6，0.4）） = -（1<em>log0.6+0</em>log0.4）=0.222<br>H2（（1，0）（0.8，0.2）） = -（1<em>log0.8+0</em>log0.2）=0.097</p><p>明显H2预测更准确</p><pre><code>ce = tf.reduce_mean(y_*log(tf.clip_by_value(y,1e-12,1.0))) </code></pre><p> y&lt;1e-12时为1e-12</p><p> y&gt;1e-12时为1</p><p>当n分类的n个输出通过softmax()函数，便满足概率分布要求</p><pre><code>ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y,labels = tf.argmax(y_,1))cem = tf.reduce_mean(ce)</code></pre><h5 id="1-8-1-3-学习率：（每次权值更新的幅度）。"><a href="#1-8-1-3-学习率：（每次权值更新的幅度）。" class="headerlink" title="1.8.1.3 学习率：（每次权值更新的幅度）。"></a>1.8.1.3 学习率：（每次权值更新的幅度）。</h5><p>W1 = w0 - learning_rate*<a href="http://www.codecogs.com/eqnedit.php?latex=\blacktriangledown" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\blacktriangledown" title="\blacktriangledown"></a>（损失函数的导数）</p><p>如：<br>损失函数loss =<a href="http://www.codecogs.com/eqnedit.php?latex=(w&plus;1)^2" target="_blank"><img src="http://latex.codecogs.com/gif.latex?(w&plus;1)^2" title="(w+1)^2"></a>,梯度<a href="http://www.codecogs.com/eqnedit.php?latex=\blacktriangledown" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\blacktriangledown" title="\blacktriangledown"></a> <a href="http://www.codecogs.com/eqnedit.php?latex==\frac{\partial&space;loss^2&space;}{\partial&space;w^2}=&space;2w&plus;2" target="_blank"><img src="http://latex.codecogs.com/gif.latex?=\frac{\partial&space;loss^2&space;}{\partial&space;w^2}=&space;2w&plus;2" title="=\frac{\partial loss^2 }{\partial w^2}= 2w+2"></a></p><p>参数初始值为5 学习率为0.2，则</p><p>1次 参数w:5 5-0.2<em>（2</em>5+2）=2.6</p><p>2次 参数w:2.6 2.6-0.2<em>（2</em>2.6+2）=1.16</p><p>2次 参数w:1.16 1.16-0.2<em>（2</em>.16+2）=0.296</p><p>2次 参数w:0.296</p><p>。。。。。。</p><h5 id="1-8-1-4-指数衰减学习率（根据轮数进行动态更新学习率）。"><a href="#1-8-1-4-指数衰减学习率（根据轮数进行动态更新学习率）。" class="headerlink" title="1.8.1.4 指数衰减学习率（根据轮数进行动态更新学习率）。"></a>1.8.1.4 指数衰减学习率（根据轮数进行动态更新学习率）。</h5><p><a href="http://www.codecogs.com/eqnedit.php?latex=\small&space;Learningrate&space;=&space;LEARNINGRATEBASE*LEARNINGRATEDECAY*(\frac{globalstep}{LEARNINGRATESTEP})" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\small&space;Learningrate&space;=&space;LEARNINGRATEBASE*LEARNINGRATEDECAY*(\frac{globalstep}{LEARNINGRATESTEP})" title="\small Learningrate = LEARNINGRATEBASE*LEARNINGRATEDECAY*(\frac{globalstep}{LEARNINGRATESTEP})"></a></p><p>LEARNINGRATEBASE学习率基数，LEARNING_RATE_DECOY学习率衰减率（0，1），多少轮更新一次 = （总那个样本数/batch_size）</p><pre><code>(global_step 用于计数，训练的轮数)global_step = tf.Variable(0,trainable = False)learning_rat = tf.train.exponential_decay(LEARNING_RATE_BASE,                                      global_step,                                     LEARNING_RATE_STEP,                                     LEARNING_RATE_DECAY,                                     staircase = True)</code></pre><h5 id="1-8-1-5-滑动平均（影子值）：-记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。针对所有参数w-b"><a href="#1-8-1-5-滑动平均（影子值）：-记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。针对所有参数w-b" class="headerlink" title="1.8.1.5 滑动平均（影子值）： 记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。针对所有参数w,b"></a>1.8.1.5 滑动平均（影子值）： 记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。针对所有参数w,b</h5><p>影子 = 衰减率<em>影子+（1-衰减率）</em>参数    影子初值 = 参数初值</p><p> 衰减率= min{MOVING_AVERAGE_DECAY,<a href="http://www.codecogs.com/eqnedit.php?latex=\inline&space;\small&space;\frac{1&plus;globalstep}{10&plus;globalstep}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\inline&space;\small&space;\frac{1&plus;globalstep}{10&plus;globalstep}" title="\small \frac{1+globalstep}{10+globalstep}"></a>}</p><p>如 ：MOVING_AVERAGE_DECAY = 0.99，w1 = 0,global_step = 0,w1的滑动平均值为0，参数w1更新为1。</p><p>w1滑动平均值（影子） = min{0.99，1/10}<em>0+（1-min(0.99,1/10)</em>1）= 0.9</p><p>轮数global_step= 100时，参数更新为10则</p><p>w1滑动平均值（影子） = min{0.99，101/110}<em>0.9+（1-min(0.99,101/110)</em>10）= 1.644</p><p>再次运行<br>w1滑动平均值 （影子）= min{0.99，101/110}<em>1.644+（1-min(0.99,101/110)</em>10）= 2.328<br>….</p><pre><code>#tf.assign()更新w，对象是variable tensorema = tf.train.ExponentialMovingAverage(LEARNING_RATE_DECAY,                                   global_step)ema#所有待优化的参数求滑动平均ema_op = ema.apply(tf.trainable_variables())with tf.control_dependencies([train_step,emo_op]):  train_op = tf.no_op(name = &apos;train&apos;)#ema.average(参数)查看某参数的滑动平均值</code></pre><h5 id="1-8-1-6-正则化缓解过拟合（利用给w加权值，弱化训练数据的噪声-一般不正则化b）。"><a href="#1-8-1-6-正则化缓解过拟合（利用给w加权值，弱化训练数据的噪声-一般不正则化b）。" class="headerlink" title="1.8.1.6 正则化缓解过拟合（利用给w加权值，弱化训练数据的噪声,一般不正则化b）。"></a>1.8.1.6 正则化缓解过拟合（利用给w加权值，弱化训练数据的噪声,一般不正则化b）。</h5><p>loss = loss(y,y_)+REGULATION*loss(w)</p><p>loss :损失函数,REGULATION :正则化权重,w:需要正则化的参数</p><pre><code>loss(s) = tf.contrib.layers.l1_regularizer(REGULATION)(w)loss(s) = tf.contrib.layers.l2_regularizer(REGULATION)(w)#把内容加到集合对应位置做加法tf.add_to_collection(&quot;losses&quot;,tf.contrib.layers.l1_regularizer(REGULATIONER)(w))  loss = cem(交叉熵)+tf.add_n(tf.get_collection(&quot;losses))</code></pre><p>正则化示例</p><pre><code>import tensorflow as tfimport randomimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltBATCH_SIZE = 30seed = 2rdn = np.random.RandomState(seed)X = rdn.randn(300,2)#如x1^2+x0^2&lt;2时为1，相反为0Y_ = [int((x0*x0+x1*x1)&lt;2) for (x0,x1) in X]Y_c = [[&apos;red&apos; if y else &apos;blue&apos;] for y in Y_]X = np.vstack(X).reshape(-1,2)Y_= np.vstack(Y_).reshape(-1,1)plt.scatter(X[0:,0],X[:,1],c = np.squeeze(Y_c))plt.show()</code></pre><p><img src="https://i.imgur.com/UTPpWbQ.png" alt=""></p><p>构建神经网络</p><pre><code>def get_weight(shape,regularizer):  w = tf.Variable(tf.random_normal(shape),dtype = tf.float32)  tf.add_to_collection(&quot;losses&quot;,tf.contrib.layers.l2_regularizer(regularizer)(w))    return wdef get_bias(shape):  b = tf.Variable(tf.constant(0.01,shape = shape))  return bx = tf.placeholder(tf.float32,shape = [None,2])y_ = tf.placeholder(tf.float32,shape =[None,1])w1 = get_weight([2,11],0.01)b1 = get_bias([11])y1 = tf.nn.relu(tf.matmul(x,w1)+b1)w2 = get_weight([11,1],0.01)b2 = get_bias([1])y = tf.matmul(y1,w2)+b2loss_mse = tf.reduce_mean(tf.square(y-y_))loss_total = loss_mse + tf.add_n(tf.get_collection(&quot;losses&quot;))#有正则化时使用loss_total,没有时使用loss_mse即可train_step = tf.train.AdadeltaOptimizer(0.0001).minimize(loss_mse)with tf.Session() as sess:    init_op = tf.global_variables_initializer()    sess.run(init_op)    STEPS = 40000    for i in range(STEPS):        start = (i*BATCH_SIZE)%200        end = start+BATCH_SIZE        sess.run(train_step,feed_dict={x:X[start:end],y_:Y_[start:end]})        if i %2000 ==0:            loss_mse_v = sess.run(loss_mse,feed_dict={x:X,y_:Y_})            print(&quot;After %d steps ,loss = %f&quot;%(i,loss_mse_v))    #（在-3到3步长为0.01）    xx,yy = np.mgrid[-3:3:.01,-3:3:.01]     grid = np.c_[xx.ravel(),yy.ravel()]    probs = sess.run(y,feed_dict={x:grid})    probs = probs .reshape(xx.shape)plt.scatter(X[:,0],X[:,1],c= np.squeeze(Y_c))plt.contour(xx,yy,probs,levels = [.5])plt.show()  </code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-tensorflow&quot;&gt;&lt;a href=&quot;#1-tensorflow&quot; class=&quot;headerlink&quot; title=&quot;1. tensorflow&quot;&gt;&lt;/a&gt;1. tensorflow&lt;/h1&gt;&lt;h2 id=&quot;1-1-神经网络-python3-5&quot;&gt;&lt;a 
      
    
    </summary>
    
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Google免费Gpu使用</title>
    <link href="http://yoursite.com/2018/10/19/%E5%85%8D%E8%B4%B9Gpu%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/10/19/免费Gpu笔记/</id>
    <published>2018-10-19T14:27:04.000Z</published>
    <updated>2018-10-20T09:32:32.613Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Google-gpu使用笔记（找不到drive文件路径）"><a href="#Google-gpu使用笔记（找不到drive文件路径）" class="headerlink" title="Google gpu使用笔记（找不到drive文件路径）"></a>Google gpu使用笔记（找不到drive文件路径）</h1><h2 id="1-创建账号，创建自己的文件夹可以上传自己的数据，文件等，关联自己的colaboratory-notebooks，设置自己的gpu版本，修改-gt-笔记本设置-gt-python版本选择，gpu选择-gt-保存"><a href="#1-创建账号，创建自己的文件夹可以上传自己的数据，文件等，关联自己的colaboratory-notebooks，设置自己的gpu版本，修改-gt-笔记本设置-gt-python版本选择，gpu选择-gt-保存" class="headerlink" title="1. 创建账号，创建自己的文件夹可以上传自己的数据，文件等，关联自己的colaboratory notebooks，设置自己的gpu版本，修改&gt;笔记本设置&gt;python版本选择，gpu选择&gt;保存"></a>1. 创建账号，创建自己的文件夹可以上传自己的数据，文件等，关联自己的colaboratory notebooks，设置自己的gpu版本，修改&gt;笔记本设置&gt;python版本选择，gpu选择&gt;保存</h2><h2 id="2-导包"><a href="#2-导包" class="headerlink" title="2. 导包"></a>2. 导包</h2><pre><code>!pip install -U -q PyDrivefrom pydrive.auth import GoogleAuthfrom pydrive.drive import GoogleDrivefrom google.colab import authfrom oauth2client.client import GoogleCredentials</code></pre><h2 id="3-申请资源"><a href="#3-申请资源" class="headerlink" title="3.申请资源"></a>3.申请资源</h2><pre><code>!apt-get install -y -qq software-properties-common python-software-properties module-init-tools!add-apt-repository -y ppa:alessandro-strada/ppa 2&gt;&amp;1 &gt; /dev/null!apt-get update -qq 2&gt;&amp;1 &gt; /dev/null!apt-get -y install -qq google-drive-ocamlfuse fusefrom google.colab import authauth.authenticate_user()from oauth2client.client import GoogleCredentialscreds = GoogleCredentials.get_application_default()import getpass!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} &lt; /dev/null 2&gt;&amp;1 | grep URLvcode = getpass.getpass()!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}</code></pre><p>   会出现验证网址点击进去复制回来确认即可</p><h2 id="4-挂载"><a href="#4-挂载" class="headerlink" title="4. 挂载"></a>4. 挂载</h2><pre><code>!mkdir -p drive!google-drive-ocamlfuse drive</code></pre><h2 id="5-查看目录下的文件"><a href="#5-查看目录下的文件" class="headerlink" title="5. 查看目录下的文件"></a>5. 查看目录下的文件</h2><pre><code>!ls drive/</code></pre><h2 id="6-上传文件"><a href="#6-上传文件" class="headerlink" title="6. 上传文件"></a>6. 上传文件</h2><pre><code>from google.colab import filesuploaded = files.upload()for fn in uploaded.keys():      print(&apos;User uploaded file &quot;{name}&quot; with length {length} bytes&apos;.format(        name=fn, length=len(uploaded[fn])))</code></pre><h2 id="7-下载文件到本地系统"><a href="#7-下载文件到本地系统" class="headerlink" title="7. 下载文件到本地系统"></a>7. 下载文件到本地系统</h2><pre><code>from google.colab import fileswith open(&apos;example.txt&apos;, &apos;w&apos;) as f:  f.write(&apos;some content&apos;)files.download(&apos;example.txt&apos;)</code></pre><h2 id="8-运行文件"><a href="#8-运行文件" class="headerlink" title="8. 运行文件"></a>8. 运行文件</h2><pre><code>!python3(your version) drive/app/mnist_cnn.py(your file_path)</code></pre><h3 id="1-文件路径设置-示例"><a href="#1-文件路径设置-示例" class="headerlink" title="1.文件路径设置(示例)"></a>1.文件路径设置(示例)</h3><pre><code>open(&quot;drive/app/animals.tfrecords&quot;)</code></pre><h2 id="9-提示"><a href="#9-提示" class="headerlink" title="9. 提示"></a>9. 提示</h2><p>每次创建一个colaboratory时都需要重新运行一次1，2，3，4步骤</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Google-gpu使用笔记（找不到drive文件路径）&quot;&gt;&lt;a href=&quot;#Google-gpu使用笔记（找不到drive文件路径）&quot; class=&quot;headerlink&quot; title=&quot;Google gpu使用笔记（找不到drive文件路径）&quot;&gt;&lt;/a&gt;Go
      
    
    </summary>
    
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
</feed>
