<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>欢迎来到我的hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-10-21T13:25:29.119Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>tensorflow笔记</title>
    <link href="http://yoursite.com/2018/10/21/tensorflow%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/10/21/tensorflow笔记/</id>
    <published>2018-10-21T07:11:20.000Z</published>
    <updated>2018-10-21T13:25:29.119Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-tensorflow"><a href="#1-tensorflow" class="headerlink" title="1. tensorflow"></a>1. tensorflow</h1><h2 id="1-1-神经网络-python3-5"><a href="#1-1-神经网络-python3-5" class="headerlink" title="1.1 神经网络(python3.5)"></a>1.1 神经网络(python3.5)</h2><h3 id="1-2-参数：权重W"><a href="#1-2-参数：权重W" class="headerlink" title="1.2 参数：权重W"></a>1.2 参数：权重W</h3><p>线上的权重w，用变量表示，随机给初始值</p><pre><code>import tensorflow as tfw = tf.Variable(tf.random_normal([2,3],stddev = 2,mean = 0,seed =1))</code></pre><p>stddev: 标准差，mean: 均值,seed: 随机种子.</p><p>tf.random_normal标准正太分布，tf.truncated_normal 去掉过大偏离点的正态分布，tf.random_uniform 平均分布<br>也可以给定值</p><pre><code>tf.zeros()#全0数组 tf.zeros([3,2],tf.int32)tf.ones()#全1数组    tf.ones([3,2],tf.int32)tf.fill()#全定值数组   tf.fill([3,2],2)tf.constant()#直接赋值  tf.constant([2.0,3.0])</code></pre><h3 id="1-3-喂数据-op，运行的节点"><a href="#1-3-喂数据-op，运行的节点" class="headerlink" title="1.3 喂数据(op，运行的节点)"></a>1.3 喂数据(op，运行的节点)</h3><pre><code>sess = tf.Session()init = tf.global_variable_initlizer()sess.run(init) x = tf.placeholder(tf.float,shape = (None,2))sess.run(op,feed_dic={x:[[0.5,0.6]]})    </code></pre><h3 id="1-4-前向传播"><a href="#1-4-前向传播" class="headerlink" title="1.4 前向传播"></a>1.4 前向传播</h3><h3 id="1-5-反向传播-更新参数"><a href="#1-5-反向传播-更新参数" class="headerlink" title="1.5 反向传播(更新参数)"></a>1.5 反向传播(更新参数)</h3><h4 id="1-5-1-损失函数"><a href="#1-5-1-损失函数" class="headerlink" title="1.5.1 损失函数"></a>1.5.1 损失函数</h4><p>预测值y与真实值y_之间的差值</p><h4 id="1-5-2-均方误差：MSE"><a href="#1-5-2-均方误差：MSE" class="headerlink" title="1.5.2 均方误差：MSE"></a>1.5.2 均方误差：MSE</h4><pre><code>loss = tf.reduce_mean(tf.square(y_-y))</code></pre><h4 id="1-5-3-训练方法：以减少loss为优化目标"><a href="#1-5-3-训练方法：以减少loss为优化目标" class="headerlink" title="1.5.3 训练方法：以减少loss为优化目标"></a>1.5.3 训练方法：以减少loss为优化目标</h4><pre><code>trin_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)train_op = tf.train.MomentumOptimizer(learning_rate).minimize(loss)train_op = tf.train.AdamOptimizer(leanring_rate).minimize(loss)</code></pre><h4 id="1-5-4-学习率"><a href="#1-5-4-学习率" class="headerlink" title="1.5.4 学习率"></a>1.5.4 学习率</h4><p>决定参数每次更新的幅度</p><h3 id="1-6-激活函数"><a href="#1-6-激活函数" class="headerlink" title="1.6 激活函数"></a>1.6 激活函数</h3><pre><code>tf.nn.relu()tf.nn.sigmoid()tf.nn.tanh()</code></pre><h3 id="1-7-NN复杂度"><a href="#1-7-NN复杂度" class="headerlink" title="1.7 NN复杂度"></a>1.7 NN复杂度</h3><p>层数 = 隐藏层的层数+1个输出层</p><p>总参数 =  总w（权重个数） +总b（偏置项个数）</p><h3 id="1-8-模型优化"><a href="#1-8-模型优化" class="headerlink" title="1.8 模型优化"></a>1.8 模型优化</h3><h4 id="1-8-1-损失函数loss，学习率learning-rate-滑动平均-ema-正则化regularization"><a href="#1-8-1-损失函数loss，学习率learning-rate-滑动平均-ema-正则化regularization" class="headerlink" title="1.8.1 损失函数loss，学习率learning_rate,滑动平均 ema,正则化regularization"></a>1.8.1 损失函数loss，学习率learning_rate,滑动平均 ema,正则化regularization</h4><h5 id="1-8-1-1-自定义损失函数"><a href="#1-8-1-1-自定义损失函数" class="headerlink" title="1.8.1.1 自定义损失函数"></a>1.8.1.1 自定义损失函数</h5><p>不知道y 与y_的大小时</p><pre><code>loss = tf.reduce_mean(tf.where(tf.greater(y,y_),COST(y -y_),PROFIT(y_-y)))</code></pre><h5 id="1-8-1-2-交叉熵-cem"><a href="#1-8-1-2-交叉熵-cem" class="headerlink" title="1.8.1.2 交叉熵 cem"></a>1.8.1.2 交叉熵 cem</h5><p>交叉熵越小，两个概率分布越近，越大，两个概率分布更远</p><pre><code>H= -Σ（y_*log(y)）</code></pre><p>如二分类答案 y = (1,0),预测y1 = (0.6,0.4),y2 = (0.9,0.2)哪个更接近正确答案呢？</p><p>H1（（1，0）（0.6，0.4）） = -（1<em>log0.6+0</em>log0.4）=0.222<br>H2（（1，0）（0.8，0.2）） = -（1<em>log0.8+0</em>log0.2）=0.097</p><p>明显H2预测更准确</p><pre><code>ce = tf.reduce_mean(y_*log(tf.clip_by_value(y,1e-12,1.0))) </code></pre><p> y&lt;1e-12时为1e-12</p><p> y&gt;1e-12时为1</p><p>当n分类的n个输出通过softmax()函数，便满足概率分布要求</p><pre><code>ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y,labels = tf.argmax(y_,1))cem = tf.reduce_mean(ce)</code></pre><h5 id="1-8-1-3-学习率：（每次权值更新的幅度）。"><a href="#1-8-1-3-学习率：（每次权值更新的幅度）。" class="headerlink" title="1.8.1.3 学习率：（每次权值更新的幅度）。"></a>1.8.1.3 学习率：（每次权值更新的幅度）。</h5><p>W1 = w0 - learning_rate*<a href="http://www.codecogs.com/eqnedit.php?latex=\blacktriangledown" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\blacktriangledown" title="\blacktriangledown"></a>（损失函数的导数）</p><p>如：<br>损失函数loss =<a href="http://www.codecogs.com/eqnedit.php?latex=(w&plus;1)^2" target="_blank"><img src="http://latex.codecogs.com/gif.latex?(w&plus;1)^2" title="(w+1)^2"></a>,梯度<a href="http://www.codecogs.com/eqnedit.php?latex=\blacktriangledown" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\blacktriangledown" title="\blacktriangledown"></a> <a href="http://www.codecogs.com/eqnedit.php?latex==\frac{\partial&space;loss^2&space;}{\partial&space;w^2}=&space;2w&plus;2" target="_blank"><img src="http://latex.codecogs.com/gif.latex?=\frac{\partial&space;loss^2&space;}{\partial&space;w^2}=&space;2w&plus;2" title="=\frac{\partial loss^2 }{\partial w^2}= 2w+2"></a></p><p>参数初始值为5 学习率为0.2，则</p><p>1次 参数w:5 5-0.2<em>（2</em>5+2）=2.6</p><p>2次 参数w:2.6 2.6-0.2<em>（2</em>2.6+2）=1.16</p><p>2次 参数w:1.16 1.16-0.2<em>（2</em>.16+2）=0.296</p><p>2次 参数w:0.296</p><p>。。。。。。</p><h5 id="1-8-1-4-指数衰减学习率（根据轮数进行动态更新学习率）。"><a href="#1-8-1-4-指数衰减学习率（根据轮数进行动态更新学习率）。" class="headerlink" title="1.8.1.4 指数衰减学习率（根据轮数进行动态更新学习率）。"></a>1.8.1.4 指数衰减学习率（根据轮数进行动态更新学习率）。</h5><p><a href="http://www.codecogs.com/eqnedit.php?latex=\small&space;Learningrate&space;=&space;LEARNINGRATEBASE*LEARNINGRATEDECAY*(\frac{globalstep}{LEARNINGRATESTEP})" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\small&space;Learningrate&space;=&space;LEARNINGRATEBASE*LEARNINGRATEDECAY*(\frac{globalstep}{LEARNINGRATESTEP})" title="\small Learningrate = LEARNINGRATEBASE*LEARNINGRATEDECAY*(\frac{globalstep}{LEARNINGRATESTEP})"></a></p><p>LEARNINGRATEBASE学习率基数，LEARNING_RATE_DECOY学习率衰减率（0，1），多少轮更新一次 = （总那个样本数/batch_size）</p><pre><code>(global_step 用于计数，训练的轮数)global_step = tf.Variable(0,trainable = False)learning_rat = tf.train.exponential_decay(LEARNING_RATE_BASE,                                      global_step,                                     LEARNING_RATE_STEP,                                     LEARNING_RATE_DECAY,                                     staircase = True)</code></pre><h5 id="1-8-1-5-滑动平均（影子值）：-记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。针对所有参数w-b"><a href="#1-8-1-5-滑动平均（影子值）：-记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。针对所有参数w-b" class="headerlink" title="1.8.1.5 滑动平均（影子值）： 记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。针对所有参数w,b"></a>1.8.1.5 滑动平均（影子值）： 记录了每个参数一段时间内过往值的平均，增加了模型的泛化性。针对所有参数w,b</h5><p>影子 = 衰减率<em>影子+（1-衰减率）</em>参数    影子初值 = 参数初值</p><p> 衰减率= min{MOVING_AVERAGE_DECAY,<a href="http://www.codecogs.com/eqnedit.php?latex=\inline&space;\small&space;\frac{1&plus;globalstep}{10&plus;globalstep}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\inline&space;\small&space;\frac{1&plus;globalstep}{10&plus;globalstep}" title="\small \frac{1+globalstep}{10+globalstep}"></a>}</p><p>如 ：MOVING_AVERAGE_DECAY = 0.99，w1 = 0,global_step = 0,w1的滑动平均值为0，参数w1更新为1。</p><p>w1滑动平均值（影子） = min{0.99，1/10}<em>0+（1-min(0.99,1/10)</em>1）= 0.9</p><p>轮数global_step= 100时，参数更新为10则</p><p>w1滑动平均值（影子） = min{0.99，101/110}<em>0.9+（1-min(0.99,101/110)</em>10）= 1.644</p><p>再次运行<br>w1滑动平均值 （影子）= min{0.99，101/110}<em>1.644+（1-min(0.99,101/110)</em>10）= 2.328<br>….</p><pre><code>#tf.assign()更新w，对象是variable tensorema = tf.train.ExponentialMovingAverage(LEARNING_RATE_DECAY,                                   global_step)ema#所有待优化的参数求滑动平均ema_op = ema.apply(tf.trainable_variables())with tf.control_dependencies([train_step,emo_op]):  train_op = tf.no_op(name = &apos;train&apos;)#ema.average(参数)查看某参数的滑动平均值</code></pre><h5 id="1-8-1-6-正则化缓解过拟合（利用给w加权值，弱化训练数据的噪声-一般不正则化b）。"><a href="#1-8-1-6-正则化缓解过拟合（利用给w加权值，弱化训练数据的噪声-一般不正则化b）。" class="headerlink" title="1.8.1.6 正则化缓解过拟合（利用给w加权值，弱化训练数据的噪声,一般不正则化b）。"></a>1.8.1.6 正则化缓解过拟合（利用给w加权值，弱化训练数据的噪声,一般不正则化b）。</h5><p>loss = loss(y,y_)+REGULATION*loss(w)</p><p>loss :损失函数,REGULATION :正则化权重,w:需要正则化的参数</p><pre><code>loss(s) = tf.contrib.layers.l1_regularizer(REGULATION)(w)loss(s) = tf.contrib.layers.l2_regularizer(REGULATION)(w)#把内容加到集合对应位置做加法tf.add_to_collection(&quot;losses&quot;,tf.contrib.layers.l1_regularizer(REGULATIONER)(w))  loss = cem(交叉熵)+tf.add_n(tf.get_collection(&quot;losses))</code></pre><p>正则化示例</p><pre><code>import tensorflow as tfimport randomimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltBATCH_SIZE = 30seed = 2rdn = np.random.RandomState(seed)X = rdn.randn(300,2)#如x1^2+x0^2&lt;2时为1，相反为0Y_ = [int((x0*x0+x1*x1)&lt;2) for (x0,x1) in X]Y_c = [[&apos;red&apos; if y else &apos;blue&apos;] for y in Y_]X = np.vstack(X).reshape(-1,2)Y_= np.vstack(Y_).reshape(-1,1)plt.scatter(X[0:,0],X[:,1],c = np.squeeze(Y_c))plt.show()</code></pre><p><img src="https://i.imgur.com/UTPpWbQ.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-tensorflow&quot;&gt;&lt;a href=&quot;#1-tensorflow&quot; class=&quot;headerlink&quot; title=&quot;1. tensorflow&quot;&gt;&lt;/a&gt;1. tensorflow&lt;/h1&gt;&lt;h2 id=&quot;1-1-神经网络-python3-5&quot;&gt;&lt;a 
      
    
    </summary>
    
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Google免费Gpu使用</title>
    <link href="http://yoursite.com/2018/10/19/%E5%85%8D%E8%B4%B9Gpu%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/10/19/免费Gpu笔记/</id>
    <published>2018-10-19T14:27:04.000Z</published>
    <updated>2018-10-20T09:32:32.613Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Google-gpu使用笔记（找不到drive文件路径）"><a href="#Google-gpu使用笔记（找不到drive文件路径）" class="headerlink" title="Google gpu使用笔记（找不到drive文件路径）"></a>Google gpu使用笔记（找不到drive文件路径）</h1><h2 id="1-创建账号，创建自己的文件夹可以上传自己的数据，文件等，关联自己的colaboratory-notebooks，设置自己的gpu版本，修改-gt-笔记本设置-gt-python版本选择，gpu选择-gt-保存"><a href="#1-创建账号，创建自己的文件夹可以上传自己的数据，文件等，关联自己的colaboratory-notebooks，设置自己的gpu版本，修改-gt-笔记本设置-gt-python版本选择，gpu选择-gt-保存" class="headerlink" title="1. 创建账号，创建自己的文件夹可以上传自己的数据，文件等，关联自己的colaboratory notebooks，设置自己的gpu版本，修改&gt;笔记本设置&gt;python版本选择，gpu选择&gt;保存"></a>1. 创建账号，创建自己的文件夹可以上传自己的数据，文件等，关联自己的colaboratory notebooks，设置自己的gpu版本，修改&gt;笔记本设置&gt;python版本选择，gpu选择&gt;保存</h2><h2 id="2-导包"><a href="#2-导包" class="headerlink" title="2. 导包"></a>2. 导包</h2><pre><code>!pip install -U -q PyDrivefrom pydrive.auth import GoogleAuthfrom pydrive.drive import GoogleDrivefrom google.colab import authfrom oauth2client.client import GoogleCredentials</code></pre><h2 id="3-申请资源"><a href="#3-申请资源" class="headerlink" title="3.申请资源"></a>3.申请资源</h2><pre><code>!apt-get install -y -qq software-properties-common python-software-properties module-init-tools!add-apt-repository -y ppa:alessandro-strada/ppa 2&gt;&amp;1 &gt; /dev/null!apt-get update -qq 2&gt;&amp;1 &gt; /dev/null!apt-get -y install -qq google-drive-ocamlfuse fusefrom google.colab import authauth.authenticate_user()from oauth2client.client import GoogleCredentialscreds = GoogleCredentials.get_application_default()import getpass!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} &lt; /dev/null 2&gt;&amp;1 | grep URLvcode = getpass.getpass()!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}</code></pre><p>   会出现验证网址点击进去复制回来确认即可</p><h2 id="4-挂载"><a href="#4-挂载" class="headerlink" title="4. 挂载"></a>4. 挂载</h2><pre><code>!mkdir -p drive!google-drive-ocamlfuse drive</code></pre><h2 id="5-查看目录下的文件"><a href="#5-查看目录下的文件" class="headerlink" title="5. 查看目录下的文件"></a>5. 查看目录下的文件</h2><pre><code>!ls drive/</code></pre><h2 id="6-上传文件"><a href="#6-上传文件" class="headerlink" title="6. 上传文件"></a>6. 上传文件</h2><pre><code>from google.colab import filesuploaded = files.upload()for fn in uploaded.keys():      print(&apos;User uploaded file &quot;{name}&quot; with length {length} bytes&apos;.format(        name=fn, length=len(uploaded[fn])))</code></pre><h2 id="7-下载文件到本地系统"><a href="#7-下载文件到本地系统" class="headerlink" title="7. 下载文件到本地系统"></a>7. 下载文件到本地系统</h2><pre><code>from google.colab import fileswith open(&apos;example.txt&apos;, &apos;w&apos;) as f:  f.write(&apos;some content&apos;)files.download(&apos;example.txt&apos;)</code></pre><h2 id="8-运行文件"><a href="#8-运行文件" class="headerlink" title="8. 运行文件"></a>8. 运行文件</h2><pre><code>!python3(your version) drive/app/mnist_cnn.py(your file_path)</code></pre><h3 id="1-文件路径设置-示例"><a href="#1-文件路径设置-示例" class="headerlink" title="1.文件路径设置(示例)"></a>1.文件路径设置(示例)</h3><pre><code>open(&quot;drive/app/animals.tfrecords&quot;)</code></pre><h2 id="9-提示"><a href="#9-提示" class="headerlink" title="9. 提示"></a>9. 提示</h2><p>每次创建一个colaboratory时都需要重新运行一次1，2，3，4步骤</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Google-gpu使用笔记（找不到drive文件路径）&quot;&gt;&lt;a href=&quot;#Google-gpu使用笔记（找不到drive文件路径）&quot; class=&quot;headerlink&quot; title=&quot;Google gpu使用笔记（找不到drive文件路径）&quot;&gt;&lt;/a&gt;Go
      
    
    </summary>
    
    
      <category term="笔记" scheme="http://yoursite.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
</feed>
